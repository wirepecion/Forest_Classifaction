{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding and Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['nforest_type_encoded'] = label_encoder.fit_transform(train_df['nforest_type'])\n",
    "\n",
    "# Define features and target\n",
    "X = train_df.drop(columns=['id', 'nforest_type', 'nforest_type_encoded'])\n",
    "y = train_df['nforest_type_encoded']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Optionally, use PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)  # Adjust n_components as needed\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Split the transformed data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)  # Convert Series to numpy array first\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.long)  # Convert Series to numpy array first\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and Train the Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.9550, Val Loss: 0.8225, Val Accuracy: 64.88%\n",
      "Epoch 2/30, Train Loss: 0.8731, Val Loss: 0.7839, Val Accuracy: 66.18%\n",
      "Epoch 3/30, Train Loss: 0.8499, Val Loss: 0.7644, Val Accuracy: 66.83%\n",
      "Epoch 4/30, Train Loss: 0.8376, Val Loss: 0.7465, Val Accuracy: 67.68%\n",
      "Epoch 5/30, Train Loss: 0.8226, Val Loss: 0.7352, Val Accuracy: 68.06%\n",
      "Epoch 6/30, Train Loss: 0.8211, Val Loss: 0.7399, Val Accuracy: 68.02%\n",
      "Epoch 7/30, Train Loss: 0.8040, Val Loss: 0.7285, Val Accuracy: 68.59%\n",
      "Epoch 8/30, Train Loss: 0.8105, Val Loss: 0.7277, Val Accuracy: 68.33%\n",
      "Epoch 9/30, Train Loss: 0.8061, Val Loss: 0.7276, Val Accuracy: 68.29%\n",
      "Epoch 10/30, Train Loss: 0.8099, Val Loss: 0.7205, Val Accuracy: 68.75%\n",
      "Epoch 11/30, Train Loss: 0.7953, Val Loss: 0.7198, Val Accuracy: 68.67%\n",
      "Epoch 12/30, Train Loss: 0.7969, Val Loss: 0.7220, Val Accuracy: 68.79%\n",
      "Epoch 13/30, Train Loss: 0.7911, Val Loss: 0.7191, Val Accuracy: 68.63%\n",
      "Epoch 14/30, Train Loss: 0.7884, Val Loss: 0.7163, Val Accuracy: 68.79%\n",
      "Epoch 15/30, Train Loss: 0.7917, Val Loss: 0.7150, Val Accuracy: 68.82%\n",
      "Epoch 16/30, Train Loss: 0.7905, Val Loss: 0.7240, Val Accuracy: 68.44%\n",
      "Epoch 17/30, Train Loss: 0.7881, Val Loss: 0.7125, Val Accuracy: 69.02%\n",
      "Epoch 18/30, Train Loss: 0.7890, Val Loss: 0.7129, Val Accuracy: 69.17%\n",
      "Epoch 19/30, Train Loss: 0.7890, Val Loss: 0.7126, Val Accuracy: 69.32%\n",
      "Epoch 20/30, Train Loss: 0.7941, Val Loss: 0.7163, Val Accuracy: 68.82%\n",
      "Epoch 21/30, Train Loss: 0.7882, Val Loss: 0.7183, Val Accuracy: 69.05%\n",
      "Epoch 22/30, Train Loss: 0.7879, Val Loss: 0.7145, Val Accuracy: 68.94%\n",
      "Epoch 23/30, Train Loss: 0.7742, Val Loss: 0.7147, Val Accuracy: 68.79%\n",
      "Epoch 24/30, Train Loss: 0.7849, Val Loss: 0.7143, Val Accuracy: 68.90%\n",
      "Epoch 25/30, Train Loss: 0.7862, Val Loss: 0.7148, Val Accuracy: 68.67%\n",
      "Epoch 26/30, Train Loss: 0.7888, Val Loss: 0.7144, Val Accuracy: 69.09%\n",
      "Epoch 27/30, Train Loss: 0.7896, Val Loss: 0.7202, Val Accuracy: 69.09%\n",
      "Epoch 28/30, Train Loss: 0.7856, Val Loss: 0.7193, Val Accuracy: 68.82%\n",
      "Epoch 29/30, Train Loss: 0.7871, Val Loss: 0.7177, Val Accuracy: 68.52%\n",
      "Epoch 30/30, Train Loss: 0.7925, Val Loss: 0.7158, Val Accuracy: 68.90%\n"
     ]
    }
   ],
   "source": [
    "class ImprovedForestNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ImprovedForestNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the neural network, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = len(label_encoder.classes_)\n",
    "model = ImprovedForestNN(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {running_loss/len(train_loader):.4f}, '\n",
    "          f'Val Loss: {val_loss/len(val_loader):.4f}, '\n",
    "          f'Val Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test data\n",
    "test_X = test_df.drop(columns=['id'])\n",
    "test_X_scaled = scaler.transform(test_X)\n",
    "test_X_pca = pca.transform(test_X_scaled)\n",
    "test_X_tensor = torch.tensor(test_X_pca, dtype=torch.float32)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(test_X_tensor)\n",
    "    _, test_predictions = torch.max(test_outputs.data, 1)\n",
    "\n",
    "test_predictions_decoded = label_encoder.inverse_transform(test_predictions.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your sample submission file\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Merge the test data with sample submission to fill in the predicted values\n",
    "predictions_df = pd.DataFrame({'id': test_df['id'], 'nforest_type': test_predictions_decoded})\n",
    "final_submission = sample_submission.merge(predictions_df, on='id', how='left', suffixes=('', '_predicted'))\n",
    "\n",
    "# Fill the missing values in sample submission with the predicted values\n",
    "final_submission['nforest_type'] = final_submission['nforest_type'].combine_first(final_submission['nforest_type_predicted'])\n",
    "\n",
    "# Drop the predicted column as it's no longer needed\n",
    "final_submission = final_submission.drop(columns=['nforest_type_predicted'])\n",
    "\n",
    "# Save the final submission\n",
    "final_submission.to_csv('/submitfile/NN++.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
